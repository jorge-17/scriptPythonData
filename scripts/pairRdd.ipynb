{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ee97939-6c67-4eef-8ac6-c15b794b179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b56d62cf-8279-483c-b489-a22cf3e0daa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipykernel_9011/1542122174.py:1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipykernel_9011/1542122174.py:1 "
     ]
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession.builder.appName(\"Create DataFrame\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ac03828-e970-4c95-a095-da9534bada62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creando un pair RDD apartir de una tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaebb623-477c-4758-a775-63a0e96c97f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "listsTupla = [('a',1),('b',2),('c',3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21063187-1287-4124-ade3-872db5a89088",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairRdd = sc.parallelize(listsTupla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59228e1f-f237-4300-a723-88e9b718eb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('c', 3)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5875ecbd-3918-4c1f-b6d6-e0675597274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "316c4788-9a49-4656-9b89-9d5b1df5052a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "range(stop) -> range object\n",
       "range(start, stop[, step]) -> range object\n",
       "\n",
       "Return an object that produces a sequence of integers from start (inclusive)\n",
       "to stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\n",
       "start defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\n",
       "These are exactly the valid indices for a list of 4 elements.\n",
       "When step is given, it specifies the increment (or decrement).\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "414a7ade-3caa-4c92-9d88-5a3b5d528b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairRdd2 = sc.parallelize(zip((['a','b','c']), range(1,4,1))) #start, stop, range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a68ccd2-b1bb-4859-b87b-c06a5448a423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('c', 3)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc6fd542-9802-432a-86dc-b0043c1bd936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usando archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfba93e7-6099-44e5-be0f-caa8d0ce23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddData = sc.textFile('df.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcc4686e-e6df-45cc-ad88-6cf2dd9bf709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"IdRep\",\"CoEtiqueta\",\"ValorEtiqueta\",\"NumEtiqueta\",\"CoCliente\",\"FolioBC\"',\n",
       " '1,01,\"01\",1,17,1234',\n",
       " '1,02,\"AA\",1,17,1234',\n",
       " '1,03,01031993,1,17,1234',\n",
       " '1,04,1111U211,1,17,1234',\n",
       " '1,01,\"02\",2,17,1234',\n",
       " '1,02,\"AB\",2,17,1234',\n",
       " '1,03,02041993,2,17,1234',\n",
       " '1,04,1711U211,2,17,1234',\n",
       " '1,01,\"03\",3,17,1234',\n",
       " '1,02,\"AC\",3,17,1234',\n",
       " '1,03,03051993,3,17,1234',\n",
       " '1,04,1721U211,3,17,1234',\n",
       " '2,01,\"01\",1,34,1235',\n",
       " '2,02,\"AA\",1,34,1235',\n",
       " '2,03,01031993,1,34,1235',\n",
       " '2,04,1111U211,1,34,1235',\n",
       " '2,01,\"02\",2,34,1235',\n",
       " '2,02,\"AB\",2,34,1235',\n",
       " '2,03,02041993,2,34,1235',\n",
       " '2,04,1711U211,2,34,1235',\n",
       " '2,01,\"03\",3,34,1235',\n",
       " '2,02,\"AC\",3,34,1235',\n",
       " '2,03,03051993,3,34,1235',\n",
       " '2,04,1721U211,3,34,1235']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "623a8b8a-313f-445e-9033-ba30b66f8fab",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moptions(header\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "df = sc.read.format(\"txt\").options(header='true', inferSchema='true', delimiter=',').load(\"df.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "602b5989-dfa8-48ee-a4cb-41e77c35fbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        SparkContext\n",
       "\u001b[0;31mString form:\u001b[0m <SparkContext master=local[*] appName=pyspark-shell>\n",
       "\u001b[0;31mFile:\u001b[0m        /usr/local/lib/python3.10/dist-packages/pyspark/context.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Main entry point for Spark functionality. A SparkContext represents the\n",
       "connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
       "broadcast variables on that cluster.\n",
       "\n",
       "When you create a new SparkContext, at least the master and app name should\n",
       "be set, either through the named parameters here or through `conf`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "master : str, optional\n",
       "    Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).\n",
       "appName : str, optional\n",
       "    A name for your job, to display on the cluster web UI.\n",
       "sparkHome : str, optional\n",
       "    Location where Spark is installed on cluster nodes.\n",
       "pyFiles : list, optional\n",
       "    Collection of .zip or .py files to send to the cluster\n",
       "    and add to PYTHONPATH.  These can be paths on the local file\n",
       "    system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
       "environment : dict, optional\n",
       "    A dictionary of environment variables to set on\n",
       "    worker nodes.\n",
       "batchSize : int, optional, default 0\n",
       "    The number of Python objects represented as a single\n",
       "    Java object. Set 1 to disable batching, 0 to automatically choose\n",
       "    the batch size based on object sizes, or -1 to use an unlimited\n",
       "    batch size\n",
       "serializer : :class:`Serializer`, optional, default :class:`CPickleSerializer`\n",
       "    The serializer for RDDs.\n",
       "conf : :class:`SparkConf`, optional\n",
       "    An object setting Spark properties.\n",
       "gateway : class:`py4j.java_gateway.JavaGateway`,  optional\n",
       "    Use an existing gateway and JVM, otherwise a new JVM\n",
       "    will be instantiated. This is only used internally.\n",
       "jsc : class:`py4j.java_gateway.JavaObject`, optional\n",
       "    The JavaSparkContext instance. This is only used internally.\n",
       "profiler_cls : type, optional, default :class:`BasicProfiler`\n",
       "    A class of custom Profiler used to do profiling\n",
       "udf_profiler_cls : type, optional, default :class:`UDFBasicProfiler`\n",
       "    A class of custom Profiler used to do udf profiling\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n",
       "the active :class:`SparkContext` before creating a new one.\n",
       "\n",
       ":class:`SparkContext` instance is not supported to share across multiple\n",
       "processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
       "Use threads instead for concurrent processing purpose.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from pyspark.context import SparkContext\n",
       ">>> sc = SparkContext('local', 'test')\n",
       ">>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n",
       "Traceback (most recent call last):\n",
       "    ...\n",
       "ValueError: ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7cef1a-d2d1-42ac-a563-efb873518bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
